{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "726e2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "import os\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84a1be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 0) Загрузка ==========\n",
    "def load_df(path: str = \".data/train.csv\") -> pd.DataFrame:\n",
    "    \"\"\"Ожидает CSV с колонками: sample;annotation\"\"\"\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    # приводим к строкам\n",
    "    df[\"sample\"] = df[\"sample\"].astype(str)\n",
    "    df[\"annotation\"] = df[\"annotation\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========== 1) Парсер annotation ==========\n",
    "def parse_annotation(cell: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"'[(0, 7, \\\"B-TYPE\\\"), ...]' -> [(0,7,'B-TYPE'), ...]; ничего не нормализуем.\"\"\"\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return []\n",
    "    try:\n",
    "        items = ast.literal_eval(cell)\n",
    "        return [(int(s), int(e), str(t)) for s, e, t in items]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ========== 2) Токенизация с индексами ==========\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    start: int  # включительно\n",
    "    end: int    # исключительно\n",
    "\n",
    "_TOKEN_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def tokenize_with_offsets(text: str) -> List[Token]:\n",
    "    \"\"\"'абрикосы 500 г' -> [Token('абрикосы',0,8), Token('500',9,12), Token('г',13,14)]\"\"\"\n",
    "    return [Token(m.group(0), m.start(), m.end()) for m in _TOKEN_RE.finditer(text or \"\")]\n",
    "\n",
    "\n",
    "# ========== 3) Спаны -> BIO (по токенам) ==========\n",
    "def spans_to_token_bio(tokens: List[Token],\n",
    "                       spans: List[Tuple[int, int, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Раздаём каждой позиции-токену BIO-метку из символьных спанов.\n",
    "    Правило пересечения: >= 1 символа.\n",
    "    \"\"\"\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for s, e, tag in spans:\n",
    "        ent_type = tag.split(\"-\", 1)[-1] if \"-\" in tag else tag\n",
    "        first = True\n",
    "        for i, t in enumerate(tokens):\n",
    "            if not (t.end <= s or e <= t.start):\n",
    "                labels[i] = f\"{'B' if first else 'I'}-{ent_type}\"\n",
    "                first = False\n",
    "    return labels\n",
    "\n",
    "\n",
    "# ========== 4) Признаки токена (окно ±2) ==========\n",
    "def _shape(w: str) -> str:\n",
    "    return \"\".join(\"X\" if c.isalpha() else \"d\" if c.isdigit() else \"_\" for c in w)\n",
    "\n",
    "def token_features(tokens: List[Token], i: int) -> Dict[str, Any]:\n",
    "    \"\"\"Базовые фичи токена + контекст ±2; без POS/DEP и без словарей.\"\"\"\n",
    "    w = tokens[i].text\n",
    "    wl = w.lower()\n",
    "    feats: Dict[str, Any] = {\n",
    "        \"w.lower\": wl,\n",
    "        \"shape\": _shape(w),\n",
    "        \"is_digit\": w.isdigit(),\n",
    "        \"has_digit\": any(c.isdigit() for c in w),\n",
    "        \"has_pct\": (\"%\" in w) or (\"процент\" in wl),\n",
    "        \"has_hyphen\": \"-\" in w,\n",
    "        \"has_dot\": \".\" in w,\n",
    "        \"has_comma\": \",\" in w,\n",
    "        \"is_latin\": bool(re.search(r\"[A-Za-z]\", w)),\n",
    "        \"is_cyrillic\": bool(re.search(r\"[А-Яа-яЁё]\", w)),\n",
    "        \"is_upper\": w.isupper(),\n",
    "        \"is_title\": w.istitle(),\n",
    "        \"len\": len(w),\n",
    "        \"len_bin\": 0 if len(w) <= 2 else 1 if len(w) <= 5 else 2 if len(w) <= 10 else 3,\n",
    "        \"BOS\": i == 0,\n",
    "        \"EOS\": i == len(tokens) - 1,\n",
    "    }\n",
    "    # окно контекста: -1, -2, +1, +2\n",
    "    def add_ctx(j: int, prefix: str):\n",
    "        wj = tokens[j].text\n",
    "        feats.update({\n",
    "            f\"{prefix}:w.lower\": wj.lower(),\n",
    "            f\"{prefix}:shape\": _shape(wj),\n",
    "            f\"{prefix}:is_upper\": wj.isupper(),\n",
    "            f\"{prefix}:has_digit\": any(c.isdigit() for c in wj),\n",
    "            f\"{prefix}:is_latin\": bool(re.search(r\"[A-Za-z]\", wj)),\n",
    "            f\"{prefix}:is_cyr\": bool(re.search(r\"[А-Яа-яЁё]\", wj)),\n",
    "        })\n",
    "    if i - 1 >= 0: add_ctx(i - 1, \"-1\")\n",
    "    if i - 2 >= 0: add_ctx(i - 2, \"-2\")\n",
    "    if i + 1 < len(tokens): add_ctx(i + 1, \"+1\")\n",
    "    if i + 2 < len(tokens): add_ctx(i + 2, \"+2\")\n",
    "    return feats\n",
    "\n",
    "def sent2features(tokens: List[Token]) -> List[Dict[str, Any]]:\n",
    "    return [token_features(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "\n",
    "# ========== 5) CRF ==========\n",
    "def init_crf(algorithm=\"lbfgs\",\n",
    "        c1=0.01,\n",
    "        c2=0.05,\n",
    "        max_iterations=150,\n",
    "        all_possible_transitions=True,\n",
    "        all_possible_states=False,\n",
    "        verbose=False,) -> CRF:\n",
    "    \"\"\"CRF для BIO, 300 итераций.\"\"\"\n",
    "    return CRF(\n",
    "        algorithm=algorithm,\n",
    "        c1=c1,\n",
    "        c2=c2,\n",
    "        max_iterations=max_iterations,\n",
    "        all_possible_transitions=all_possible_transitions,\n",
    "        all_possible_states=all_possible_states,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def build_xy(df: pd.DataFrame):\n",
    "    \"\"\"Из df(sample, annotation) делает X (фичи по токенам) и y (BIO по токенам).\"\"\"\n",
    "    X, y = [], []\n",
    "    for row in df.itertuples(index=False):\n",
    "        text = str(row.sample)\n",
    "        spans = parse_annotation(row.annotation)\n",
    "        toks = tokenize_with_offsets(text)\n",
    "        X.append(sent2features(toks))\n",
    "        y.append(spans_to_token_bio(toks, spans))\n",
    "    return X, y\n",
    "\n",
    "def fit_crf(crf: CRF, X, y) -> CRF:\n",
    "    crf.fit(X, y)\n",
    "    return crf\n",
    "\n",
    "\n",
    "# ========== 6) BIO -> символьные спаны ==========\n",
    "def bio_to_spans(tokens: List[Token], labels: List[str]) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"Склеиваем предсказанные BIO в интервалы (start,end,'B-ENTITY') в координатах исходного текста.\"\"\"\n",
    "    spans = []\n",
    "    cur_ent, cur_s, cur_e = None, None, None\n",
    "\n",
    "    def push():\n",
    "        if cur_ent is not None and cur_s is not None and cur_e is not None:\n",
    "            spans.append((cur_s, cur_e, f\"B-{cur_ent}\"))\n",
    "\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if not lab or lab == \"O\" or \"-\" not in lab:\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "                cur_ent, cur_s, cur_e = None, None, None\n",
    "            continue\n",
    "        pref, ent = lab.split(\"-\", 1)\n",
    "        if pref == \"B\":\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "            cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "        elif pref == \"I\":\n",
    "            if cur_ent == ent:\n",
    "                cur_e = tok.end\n",
    "            else:\n",
    "                if cur_ent is not None:\n",
    "                    push()\n",
    "                cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "    if cur_ent is not None:\n",
    "        push()\n",
    "    return spans\n",
    "\n",
    "\n",
    "# ========== 7) Инференс одной строки ==========\n",
    "def predict_one(crf: CRF, text: str) -> List[Tuple[int, int, str]]:\n",
    "    toks = tokenize_with_offsets(text)\n",
    "    if not toks:\n",
    "        return []\n",
    "    y_hat = crf.predict([sent2features(toks)])[0]\n",
    "    return bio_to_spans(toks, y_hat)\n",
    "\n",
    "\n",
    "# ========== 8) Метрика strict spans macro-F1 ==========\n",
    "def spans_exact_f1(y_true: List[List[Tuple[int,int,str]]],\n",
    "                   y_pred: List[List[Tuple[int,int,str]]]) -> float:\n",
    "    \"\"\"TP/FP/FN считаем по ПОЛНОМУ совпадению (start,end, type). Усреднение по TYPE/BRAND/VOLUME/PERCENT.\"\"\"\n",
    "    types = [\"TYPE\", \"BRAND\", \"VOLUME\", \"PERCENT\"]\n",
    "    f1s = []\n",
    "    for ent in types:\n",
    "        tp = fp = fn = 0\n",
    "        for t_sp, p_sp in zip(y_true, y_pred):\n",
    "            T = {(s, e) for s, e, tag in t_sp if str(tag).endswith(ent)}\n",
    "            P = {(s, e) for s, e, tag in p_sp if str(tag).endswith(ent)}\n",
    "            I = T & P\n",
    "            tp += len(I); fp += len(P - I); fn += len(T - I)\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "\n",
    "# ========== 9) Утилита для просмотра ==========\n",
    "def spans_text_view(text: str, spans):\n",
    "    parts = []\n",
    "    for s, e, t in sorted(spans, key=lambda z: (z[0], z[1])):\n",
    "        frag = text[s:e].replace(\"\\n\", \"\\\\n\")\n",
    "        ent = t.split(\"-\", 1)[-1] if \"-\" in t else t\n",
    "        parts.append(f\"{ent}='{frag}'[{s}:{e}]\")\n",
    "    return \" | \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba07ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(crf: CRF, path: str, extra_meta: Dict[str, Any] | None = None) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    payload = {\n",
    "        \"model\": crf,\n",
    "        \"meta\": {\n",
    "            \"algo\": \"CRF-lbfgs\",\n",
    "            \"c1\": crf.c1, \"c2\": crf.c2,\n",
    "            \"max_iter\": crf.max_iterations,\n",
    "            \"all_possible_transitions\": crf.all_possible_transitions,\n",
    "            **(extra_meta or {}),\n",
    "        }\n",
    "    }\n",
    "    dump(payload, path)\n",
    "\n",
    "def load_model(path: str) -> CRF:\n",
    "    payload = load(path)\n",
    "    return payload[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ab95092",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \".data/train.csv\"\n",
    "MODEL_PATH = \"models/crf_baseline_2.joblib\" \n",
    "\n",
    "df = load_df(\".data/train.csv\")\n",
    "df = df[df[\"sample\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "tr_df, va_df = train_test_split(df, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "X_tr, y_tr = build_xy(tr_df)\n",
    "X_va, y_va = build_xy(va_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebeacd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = init_crf(max_iterations=300)\n",
    "fit_crf(crf, X_tr, y_tr)\n",
    "save_model(crf, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce13b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro-F1 (strict spans): 0.7843\n"
     ]
    }
   ],
   "source": [
    "y_true_sp = [parse_annotation(a) for a in va_df[\"annotation\"]]\n",
    "y_pred_sp = [predict_one(crf, s) for s in va_df[\"sample\"]]\n",
    "f1 = spans_exact_f1(y_true_sp, y_pred_sp)\n",
    "print(f\"Validation macro-F1 (strict spans): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ead3a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_list(text: str, spans):\n",
    "    \"\"\"[(s,e,tag)] -> [(s, e, 'TAG')] с сохранёнными тегами как есть.\"\"\"\n",
    "    # просто убеждаемся, что это список кортежей правильного вида\n",
    "    out = []\n",
    "    for s, e, t in spans:\n",
    "        out.append((int(s), int(e), str(t)))\n",
    "    return out\n",
    "\n",
    "def print_sample(text: str, gold_spans, pred_spans):\n",
    "    print(f\"sample: {text}\")\n",
    "    print(f\"y: {spans_list(text, gold_spans)}\")\n",
    "    print(f\"pred: {spans_list(text, pred_spans)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "446ee16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Samples ===\n",
      "----------------------------------------------------------------------\n",
      "sample: лсвежитель\n",
      "y: [(0, 10, 'B-TYPE')]\n",
      "pred: [(0, 10, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: варенец останкинск\n",
      "y: [(0, 7, 'B-TYPE'), (8, 18, 'B-BRAND')]\n",
      "pred: [(0, 7, 'B-TYPE'), (8, 18, 'B-BRAND')]\n",
      "----------------------------------------------------------------------\n",
      "sample: кабачковая икра\n",
      "y: [(0, 10, 'B-TYPE'), (11, 15, 'I-TYPE')]\n",
      "pred: [(0, 10, 'B-TYPE'), (11, 15, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: фитики\n",
      "y: [(0, 6, 'B-TYPE')]\n",
      "pred: [(0, 6, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: ресень\n",
      "y: [(0, 6, 'B-TYPE')]\n",
      "pred: [(0, 6, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: столичный салато\n",
      "y: [(0, 9, 'B-BRAND'), (10, 16, 'B-TYPE')]\n",
      "pred: [(0, 9, 'B-BRAND'), (10, 16, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: виноград сады при\n",
      "y: [(0, 8, 'B-TYPE'), (9, 13, 'O'), (14, 17, 'O')]\n",
      "pred: [(0, 8, 'B-TYPE'), (9, 13, 'B-BRAND'), (14, 17, 'B-BRAND')]\n",
      "----------------------------------------------------------------------\n",
      "sample: ваиные\n",
      "y: [(0, 6, 'B-TYPE')]\n",
      "pred: [(0, 6, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: йогурт много манго\n",
      "y: [(0, 6, 'B-TYPE'), (7, 12, 'O'), (13, 18, 'O')]\n",
      "pred: [(0, 6, 'B-TYPE'), (7, 12, 'B-TYPE'), (13, 18, 'B-TYPE')]\n",
      "----------------------------------------------------------------------\n",
      "sample: печенье с цукатами\n",
      "y: [(0, 7, 'B-TYPE'), (8, 9, 'O'), (10, 18, 'O')]\n",
      "pred: [(0, 7, 'B-TYPE'), (8, 9, 'B-O'), (10, 18, 'B-O')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Samples ===\")\n",
    "for i in range(min(10, len(va_df))):\n",
    "    text = va_df.iloc[i][\"sample\"]\n",
    "    gold = y_true_sp[i]\n",
    "    pred = y_pred_sp[i]\n",
    "    print(\"-\" * 70)\n",
    "    print_sample(text, gold, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7904f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
