{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726e2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a1be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 1) Чтение train =========\n",
    "\n",
    "def read_train_semicolon(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "    assert {'sample', 'annotation'}.issubset(df.columns), \"Ожидаются колонки: sample; annotation\"\n",
    "    df = df.rename(columns={'sample': 'search_query'})\n",
    "    # приводим к строкам\n",
    "    df['search_query'] = df['search_query'].astype(str)\n",
    "    df['annotation'] = df['annotation'].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========= 2) Парсер annotation =========\n",
    "\n",
    "def normalize_tag(tag: str) -> str:\n",
    "    t = str(tag).strip().upper()\n",
    "    # на всякий, если встретятся редкие опечатки\n",
    "    t = t.replace(\"IBRAND\", \"I-BRAND\").replace(\"BBRAND\", \"B-BRAND\")\n",
    "    # оставляем только допустимые типы\n",
    "    return t\n",
    "\n",
    "def parse_annotation(cell: str) -> List[Tuple[int, int, str]]:\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return []\n",
    "    try:\n",
    "        items = ast.literal_eval(cell)\n",
    "        out = []\n",
    "        for it in items:\n",
    "            if isinstance(it, (list, tuple)) and len(it) == 3:\n",
    "                s, e, t = it\n",
    "                out.append((int(s), int(e), normalize_tag(t)))\n",
    "        return out\n",
    "    except Exception:\n",
    "        tuples = re.findall(r\"\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*'([^']+)'\\s*\\)\", cell)\n",
    "        return [(int(s), int(e), normalize_tag(t)) for s, e, t in tuples]\n",
    "\n",
    "\n",
    "# ========= 3) Токены с индексами =========\n",
    "\n",
    "_token_re = re.compile(r'\\S+')  # последовательности непробельных символов\n",
    "\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    start: int  # включительно\n",
    "    end: int    # исключительно\n",
    "\n",
    "def tokenize_with_offsets(text: str) -> List[Token]:\n",
    "    return [Token(m.group(0), m.start(), m.end()) for m in _token_re.finditer(text)]\n",
    "\n",
    "\n",
    "# ========= 4) BIO по токенам из символьных спанов =========\n",
    "\n",
    "ALL_ENTITY_TYPES = [\"TYPE\", \"BRAND\", \"VOLUME\", \"PERCENT\"]\n",
    "\n",
    "def spans_to_token_bio(tokens: List[Token], spans: List[Tuple[int, int, str]]) -> List[str]:\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    # нормализуем и уберём мусор\n",
    "    cleaned = []\n",
    "    for s, e, tag in spans:\n",
    "        tag = normalize_tag(tag)\n",
    "        if not re.fullmatch(r'[BI]-(TYPE|BRAND|VOLUME|PERCENT)', tag):\n",
    "            continue\n",
    "        ent = tag.split('-', 1)[1]\n",
    "        cleaned.append((s, e, ent))\n",
    "\n",
    "    for s, e, ent in cleaned:\n",
    "        first = True\n",
    "        for i, tok in enumerate(tokens):\n",
    "            # пересечение интервалов [tok.start,tok.end) и [s,e)\n",
    "            if not (tok.end <= s or e <= tok.start):\n",
    "                labels[i] = f\"{'B' if first else 'I'}-{ent}\"\n",
    "                first = False\n",
    "    return labels\n",
    "\n",
    "\n",
    "# ========= 5) Фичи для CRF (простые, но рабочие) =========\n",
    "\n",
    "def shape(word: str) -> str:\n",
    "    out = []\n",
    "    for ch in word:\n",
    "        if ch.isalpha():\n",
    "            out.append('X')\n",
    "        elif ch.isdigit():\n",
    "            out.append('d')\n",
    "        else:\n",
    "            out.append('_')\n",
    "    return ''.join(out)\n",
    "\n",
    "def token2features(sent_tokens: List[Token], i: int) -> Dict[str, Any]:\n",
    "    w = sent_tokens[i].text\n",
    "    wl = w.lower()\n",
    "    feats = {\n",
    "        \"bias\": 1.0,\n",
    "        \"w.lower\": wl,\n",
    "        \"w[-3:]\": wl[-3:],\n",
    "        \"w[-2:]\": wl[-2:],\n",
    "        \"w[:2]\": wl[:2],\n",
    "        \"w[:3]\": wl[:3],\n",
    "        \"shape\": shape(w),\n",
    "        \"is_alpha\": w.isalpha(),\n",
    "        \"is_digit\": w.isdigit(),\n",
    "        \"has_digit\": any(c.isdigit() for c in w),\n",
    "        \"has_pct\": ('%' in w) or ('процент' in wl),\n",
    "        \"has_comma\": (',' in w),\n",
    "        \"has_dot\": ('.' in w),\n",
    "        \"len\": len(w),\n",
    "        \"is_latin\": bool(re.search(r'[A-Za-z]', w)),\n",
    "        \"has_hyphen\": '-' in w,\n",
    "    }\n",
    "    if i > 0:\n",
    "        w1 = sent_tokens[i-1].text\n",
    "        feats.update({\"-1:w.lower\": w1.lower(), \"-1:shape\": shape(w1)})\n",
    "    else:\n",
    "        feats[\"BOS\"] = True\n",
    "    if i < len(sent_tokens)-1:\n",
    "        w1 = sent_tokens[i+1].text\n",
    "        feats.update({\"+1:w.lower\": w1.lower(), \"+1:shape\": shape(w1)})\n",
    "    else:\n",
    "        feats[\"EOS\"] = True\n",
    "    return feats\n",
    "\n",
    "def sent2features(tokens: List[Token]) -> List[Dict[str, Any]]:\n",
    "    return [token2features(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "\n",
    "# ========= 6) Регулярки чисел (VOLUME/PERCENT) =========\n",
    "# Простые и понятные: число (., или ,), потом опциональный пробел/дефис, дальше единицы с разными формами.\n",
    "VOLUME_RE = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?<!\\w)\n",
    "    (?P<num>\\d+(?:[.,]\\d+)?)\n",
    "    \\s*[-]?\\s*\n",
    "    (?P<u>\n",
    "        мл|миллилитр\\w*|\n",
    "        л|литр\\w*|\n",
    "        г|гр|грамм\\w*|\n",
    "        кг|\n",
    "        шт|штук\\w*\n",
    "    )\n",
    "    \\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "PERCENT_RE = re.compile(\n",
    "    r\"\"\"(?ix)\n",
    "    (?<!\\w)\n",
    "    (?:\n",
    "        (?P<num>\\d+(?:[.,]\\d+)?)\\s*%      # 2.5%\n",
    "        |\n",
    "        (?P<numw>\\d+(?:[.,]\\d+)?)\\s*\n",
    "        (процент\\w*)                      # 2,5 процента / процентов\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def find_rule_spans(text: str) -> List[Tuple[int, int, str]]:\n",
    "    out = []\n",
    "    for m in VOLUME_RE.finditer(text):\n",
    "        out.append((m.start(), m.end(), \"B-VOLUME\"))\n",
    "    for m in PERCENT_RE.finditer(text):\n",
    "        out.append((m.start(), m.end(), \"B-PERCENT\"))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ========= 7) Корпус для обучения =========\n",
    "\n",
    "@dataclass\n",
    "class SentExample:\n",
    "    tokens: List[Token]\n",
    "    labels: List[str]\n",
    "\n",
    "def build_corpus(df: pd.DataFrame) -> List[SentExample]:\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"search_query\"]\n",
    "        spans = parse_annotation(row[\"annotation\"])\n",
    "        toks = tokenize_with_offsets(q)\n",
    "        labs = spans_to_token_bio(toks, spans)\n",
    "        data.append(SentExample(tokens=toks, labels=labs))\n",
    "    return data\n",
    "\n",
    "\n",
    "# ========= 8) Обучение CRF =========\n",
    "\n",
    "def train_crf(train_data: List[SentExample]) -> CRF:\n",
    "    X = [sent2features(s.tokens) for s in train_data]\n",
    "    y = [s.labels for s in train_data]\n",
    "    crf = CRF(\n",
    "        algorithm=\"lbfgs\",\n",
    "        c1=0.1, c2=0.1,\n",
    "        max_iterations=150,\n",
    "        all_possible_transitions=True,\n",
    "    )\n",
    "    crf.fit(X, y)\n",
    "    return crf\n",
    "\n",
    "\n",
    "# ========= 9) BIO -> символьные спаны =========\n",
    "\n",
    "def bio_to_spans(tokens: List[Token], labels: List[str]) -> List[Tuple[int, int, str]]:\n",
    "    spans = []\n",
    "    cur_ent, cur_s, cur_e = None, None, None\n",
    "\n",
    "    def push():\n",
    "        if cur_ent is not None and cur_s is not None and cur_e is not None:\n",
    "            spans.append((cur_s, cur_e, f\"B-{cur_ent}\"))\n",
    "\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if lab == \"O\" or lab is None:\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "                cur_ent, cur_s, cur_e = None, None, None\n",
    "            continue\n",
    "\n",
    "        if '-' not in lab:\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "                cur_ent, cur_s, cur_e = None, None, None\n",
    "            continue\n",
    "\n",
    "        pref, ent = lab.split('-', 1)\n",
    "        if pref == \"B\":\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "            cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "        elif pref == \"I\":\n",
    "            if cur_ent == ent:\n",
    "                cur_e = tok.end\n",
    "            else:\n",
    "                if cur_ent is not None:\n",
    "                    push()\n",
    "                cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "\n",
    "    if cur_ent is not None:\n",
    "        push()\n",
    "    return spans\n",
    "\n",
    "\n",
    "# ========= 10) Инференс одной строки =========\n",
    "\n",
    "def predict_spans(crf: CRF, text: str) -> List[Tuple[int, int, str]]:\n",
    "    tokens = tokenize_with_offsets(text)\n",
    "    rule_spans = find_rule_spans(text)\n",
    "    X = [sent2features(tokens)]\n",
    "    y_hat = crf.predict(X)[0] if tokens else []\n",
    "    model_spans = bio_to_spans(tokens, y_hat)\n",
    "\n",
    "    # Числовым сущностям даём приоритет правил\n",
    "    final_spans = []\n",
    "    def is_num(tag): return tag.endswith(\"VOLUME\") or tag.endswith(\"PERCENT\")\n",
    "\n",
    "    for s, e, t in model_spans:\n",
    "        ent = t.split('-', 1)[1]\n",
    "        if ent in (\"VOLUME\", \"PERCENT\"):\n",
    "            if any(not (e2 <= s or e <= s2) for s2, e2, t2 in rule_spans if t2.endswith(ent)):\n",
    "                continue\n",
    "        final_spans.append((s, e, f\"B-{ent}\"))\n",
    "\n",
    "    final_spans.extend(rule_spans)\n",
    "    final_spans.sort(key=lambda z: (z[0], z[1]))\n",
    "\n",
    "    # Склейка одинаковых пересекающихся спанов\n",
    "    merged = []\n",
    "    for s, e, t in final_spans:\n",
    "        if not merged:\n",
    "            merged.append([s, e, t])\n",
    "        else:\n",
    "            ps, pe, pt = merged[-1]\n",
    "            if pt == t and s <= pe:\n",
    "                merged[-1][1] = max(pe, e)\n",
    "            else:\n",
    "                merged.append([s, e, t])\n",
    "    return [(s, e, t) for s, e, t in merged]\n",
    "\n",
    "\n",
    "# ========= 11) F1 по спанам (строгий) =========\n",
    "\n",
    "def spans_exact_f1(y_true: List[List[Tuple[int,int,str]]],\n",
    "                   y_pred: List[List[Tuple[int,int,str]]]) -> float:\n",
    "    types = [\"TYPE\", \"BRAND\", \"VOLUME\", \"PERCENT\"]\n",
    "    f1s = []\n",
    "    for ent in types:\n",
    "        tp = fp = fn = 0\n",
    "        for t_sp, p_sp in zip(y_true, y_pred):\n",
    "            t = {(s, e) for s, e, tag in t_sp if tag.endswith(ent)}\n",
    "            p = {(s, e) for s, e, tag in p_sp if tag.endswith(ent)}\n",
    "            inter = t & p\n",
    "            tp += len(inter); fp += len(p - inter); fn += len(t - inter)\n",
    "        prec = tp/(tp+fp) if tp+fp else 0.0\n",
    "        rec  = tp/(tp+fn) if tp+fn else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if prec+rec else 0.0\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab95092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation macro-F1: 0.6757\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = \".data/train.csv\"\n",
    "\n",
    "df = read_train_semicolon(TRAIN_PATH)\n",
    "df = df[df[\"search_query\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# Парсим спаны\n",
    "df[\"spans\"] = df[\"annotation\"].apply(parse_annotation)\n",
    "\n",
    "# Разбиваем train -> train/valid\n",
    "tr_df, va_df = train_test_split(df, test_size=0.15, random_state=42, shuffle=True)\n",
    "\n",
    "# Собираем корпус\n",
    "tr_data = build_corpus(tr_df)\n",
    "va_data = build_corpus(va_df)\n",
    "\n",
    "# Учим CRF\n",
    "crf = train_crf(tr_data)\n",
    "\n",
    "# Оцениваем\n",
    "va_true_spans = va_df[\"spans\"].tolist()\n",
    "va_pred_spans = []\n",
    "for text in va_df[\"search_query\"].tolist():\n",
    "    va_pred_spans.append(predict_spans(crf, text))\n",
    "f1 = spans_exact_f1(va_true_spans, va_pred_spans)\n",
    "print(f\"Validation macro-F1: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
