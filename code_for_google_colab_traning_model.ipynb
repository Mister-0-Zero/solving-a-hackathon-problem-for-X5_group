{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb25b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.44.2 seqeval==1.2.2 pytorch-crf==0.7.2 accelerate==0.34.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4389866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, ast, json, math, time, random, logging\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7237af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Конфиг (2 эпохи, без отдельного валида) ---\n",
    "SEED = 42\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "EPOCHS = 3                      # как просили\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 8\n",
    "LR = 3e-5\n",
    "MAX_LEN = 96\n",
    "WARMUP_RATIO = 0.1\n",
    "GRAD_CLIP = 1.0\n",
    "BIAS_SCALE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути (сохраняем в .data, читаем train и используем его же для инференса)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "TRAIN_CSV = \"/content/drive/MyDrive/train.csv\"         # sample;annotation\n",
    "OUT_DIR   = \"/content/drive/MyDrive/var_model_3/\"\n",
    "SUB_IN  = \"/content/drive/MyDrive/submission.csv\"\n",
    "SUB_OUT = \"/content/drive/MyDrive/sub_folder/submission.csv\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(SUB_OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7091daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BIO схема\n",
    "LABELS = [\n",
    "    \"O\",\n",
    "    \"B-BRAND\",\"I-BRAND\",\n",
    "    \"B-TYPE\",\"I-TYPE\",\n",
    "    \"B-VOLUME\",\"I-VOLUME\",\n",
    "    \"B-PERCENT\",\"I-PERCENT\",\n",
    "]\n",
    "LABEL2ID = {l:i for i,l in enumerate(LABELS)}\n",
    "ID2LABEL = {i:l for l,i in LABEL2ID.items()}\n",
    "ENTITY_TAGS = {\"BRAND\",\"TYPE\",\"VOLUME\",\"PERCENT\"}\n",
    "\n",
    "# Кого оверсемплить\n",
    "MINORITY_CLASSES = {\"VOLUME\",\"PERCENT\"}\n",
    "MINORITY_BOOST   = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19433855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=';', dtype=str, keep_default_na=False)\n",
    "    assert \"sample\" in df.columns and \"annotation\" in df.columns, \"Ожидаю колонки sample;annotation\"\n",
    "    return df\n",
    "\n",
    "def safe_parse_annotation(s: str) -> List[Tuple[int,int,str]]:\n",
    "    if s is None or str(s).strip() == \"\" or str(s).lower() in {\"nan\",\"none\",\"null\"}:\n",
    "        return []\n",
    "    txt = str(s).strip()\n",
    "    out: List[Tuple[int,int,str]] = []\n",
    "    def _push(a,b,t):\n",
    "        try:\n",
    "            a,b = int(a), int(b)\n",
    "            if b > a:\n",
    "                if t == \"O\":\n",
    "                    out.append((a,b,\"O\")); return\n",
    "                tag = str(t).split(\"-\")[-1].upper()\n",
    "                if tag in ENTITY_TAGS:\n",
    "                    pref = str(t).split(\"-\")[0] if \"-\" in str(t) else \"B\"\n",
    "                    pref = \"I\" if pref.upper().startswith(\"I\") else \"B\"\n",
    "                    out.append((a,b,f\"{pref}-{tag}\"))\n",
    "        except: pass\n",
    "    # literal_eval\n",
    "    try:\n",
    "        v = ast.literal_eval(txt)\n",
    "        if isinstance(v, list):\n",
    "            for it in v:\n",
    "                if isinstance(it, (list,tuple)) and len(it)>=3:\n",
    "                    _push(it[0], it[1], it[2])\n",
    "                elif isinstance(it, dict):\n",
    "                    _push(it.get(\"start\",0), it.get(\"end\",0), it.get(\"label\", it.get(\"tag\",\"O\")))\n",
    "            return sorted(out, key=lambda z:(z[0],z[1]))\n",
    "    except: pass\n",
    "    # json\n",
    "    try:\n",
    "        v = json.loads(txt)\n",
    "        if isinstance(v, list):\n",
    "            for it in v:\n",
    "                if isinstance(it, dict):\n",
    "                    _push(it.get(\"start\",0), it.get(\"end\",0), it.get(\"label\", it.get(\"tag\",\"O\")))\n",
    "        return sorted(out, key=lambda z:(z[0],z[1]))\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "WS_RE = re.compile(r\"\\S+\")\n",
    "def ws_tokens_with_offsets(text: str) -> List[Tuple[str,int,int]]:\n",
    "    return [(m.group(0), m.start(), m.end()) for m in WS_RE.finditer(text or \"\")]\n",
    "\n",
    "def spans_to_bio_on_words(text: str, spans: List[Tuple[int,int,str]]) -> List[str]:\n",
    "    toks = ws_tokens_with_offsets(text)\n",
    "    labels = [\"O\"]*len(toks)\n",
    "    for i, (_, ts, te) in enumerate(toks):\n",
    "        best_iou, best_tag = 0.0, \"O\"\n",
    "        for (a,b,t) in spans:\n",
    "            inter = max(0, min(te,b)-max(ts,a))\n",
    "            union = max(te,b)-min(ts,a)\n",
    "            iou = inter/union if union>0 else 0.0\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_tag = t.split(\"-\")[-1] if t!=\"O\" else \"O\"\n",
    "        if best_iou > 0 and best_tag in ENTITY_TAGS:\n",
    "            labels[i] = (\"I-\" if i>0 and labels[i-1].endswith(best_tag) else \"B-\") + best_tag\n",
    "    return labels\n",
    "\n",
    "def contains_minority(spans: List[Tuple[int,int,str]]) -> bool:\n",
    "    for _,_,lab in spans:\n",
    "        if lab!=\"O\" and lab.split(\"-\")[-1] in MINORITY_CLASSES:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "train_df = read_csv(TRAIN_CSV)\n",
    "train_df[\"__spans\"] = train_df[\"annotation\"].map(safe_parse_annotation)\n",
    "train_df[\"__has_minority\"] = train_df[\"__spans\"].map(contains_minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86dc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ячейка 3. Датасет/коллатор с мягкими «нуджами» и оверсемплингом =========\n",
    "UNITS = {\"л\",\"литр\",\"литра\",\"литров\",\"мл\",\"гр\",\"г\",\"кг\",\"шт\",\"килограммов\", \"грамм\", \"граммов\", \"миллилитров\"}\n",
    "PCT_WORDS = {\"%\",\"процент\",\"проц\", \"процентов\"}\n",
    "NUM_RE = re.compile(r\"^\\d+[\\d,.]*$\"); ASCII_RE = re.compile(r\"^[A-Za-z]+$\")\n",
    "\n",
    "def token_feature_bias(words: List[str]) -> List[Dict[str,float]]:\n",
    "    feats = []\n",
    "    for i,w in enumerate(words):\n",
    "        wl = w.lower()\n",
    "        f: Dict[str,float] = {}\n",
    "        is_num   = bool(NUM_RE.match(wl))\n",
    "        is_ascii = bool(ASCII_RE.match(w))\n",
    "        prev = words[i-1].lower() if i-1>=0 else \"\"\n",
    "        nxt  = words[i+1].lower() if i+1<len(words) else \"\"\n",
    "        if is_ascii and wl not in UNITS:\n",
    "            f[\"B-BRAND\"] = f.get(\"B-BRAND\", 0.0) + 0.25\n",
    "        if is_num and (nxt in PCT_WORDS or prev in PCT_WORDS or \"%\" in nxt or \"%\" in prev):\n",
    "            f[\"B-PERCENT\"] = f.get(\"B-PERCENT\", 0.0) + 0.35\n",
    "            f[\"I-PERCENT\"] = f.get(\"I-PERCENT\", 0.0) + 0.15\n",
    "        if is_num and (nxt in UNITS):\n",
    "            f[\"B-VOLUME\"] = f.get(\"B-VOLUME\", 0.0) + 0.35\n",
    "        if wl in UNITS and bool(NUM_RE.match(prev)):\n",
    "            f[\"I-VOLUME\"] = f.get(\"I-VOLUME\", 0.0) + 0.20\n",
    "        feats.append(f)\n",
    "    return feats\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, tok: AutoTokenizer, max_len: int):\n",
    "        self.tok = tok; self.max_len = max_len\n",
    "        self.texts = frame[\"sample\"].astype(str).tolist()\n",
    "        self.spans = frame[\"__spans\"].tolist()\n",
    "        self.ws_tokens = [[t for t,_,_ in ws_tokens_with_offsets(x)] for x in self.texts]\n",
    "        self.ws_labels = [spans_to_bio_on_words(x, s) for x,s in zip(self.texts, self.spans)]\n",
    "        self.biases    = [token_feature_bias(ws) for ws in self.ws_tokens]\n",
    "        self.has_minority = frame[\"__has_minority\"].tolist()\n",
    "        assert all(len(a)==len(b) for a,b in zip(self.ws_tokens, self.ws_labels)), \"Длины слов/меток не совпали\"\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        words, labs, feats = self.ws_tokens[idx], self.ws_labels[idx], self.biases[idx]\n",
    "        enc = self.tok(words, is_split_into_words=True, truncation=True, max_length=self.max_len)\n",
    "        word_ids = enc.word_ids()\n",
    "        aligned_labels, aligned_bias = [], []\n",
    "        prev = None\n",
    "        for wi in word_ids:\n",
    "            if wi is None:\n",
    "                aligned_labels.append(-100); aligned_bias.append([0.0]*len(LABELS))\n",
    "            elif wi != prev:\n",
    "                aligned_labels.append(LABEL2ID.get(labs[wi], 0))\n",
    "                vec = [0.0]*len(LABELS)\n",
    "                for k,v in feats[wi].items():\n",
    "                    if k in LABEL2ID: vec[LABEL2ID[k]] += v\n",
    "                aligned_bias.append(vec)\n",
    "            else:\n",
    "                aligned_labels.append(-100); aligned_bias.append([0.0]*len(LABELS))\n",
    "            prev = wi\n",
    "        enc[\"labels\"]    = aligned_labels\n",
    "        enc[\"feat_bias\"] = aligned_bias\n",
    "        return enc\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, tok): self.tok = tok\n",
    "    def __call__(self, batch):\n",
    "        model_inputs, labels_list, bias_list = [], [], []\n",
    "        for ex in batch:\n",
    "            labels_list.append(ex.pop(\"labels\"))\n",
    "            bias_list.append(ex.pop(\"feat_bias\"))\n",
    "            model_inputs.append(ex)\n",
    "        padded = self.tok.pad(model_inputs, padding=True, return_tensors=\"pt\")\n",
    "        T = padded[\"input_ids\"].size(1); B = len(labels_list); C = len(LABELS)\n",
    "        labels = torch.full((B,T), -100, dtype=torch.long)\n",
    "        bias   = torch.zeros((B,T,C), dtype=torch.float32)\n",
    "        for i,(lab,fb) in enumerate(zip(labels_list, bias_list)):\n",
    "            L = min(len(lab), T)\n",
    "            labels[i,:L] = torch.tensor(lab[:L], dtype=torch.long)\n",
    "            fb_arr = torch.tensor(fb, dtype=torch.float32)\n",
    "            Lb = min(fb_arr.shape[0], T)\n",
    "            bias[i,:Lb,:C] = fb_arr[:Lb,:C]\n",
    "        padded[\"labels\"]    = labels\n",
    "        padded[\"feat_bias\"] = bias\n",
    "        return padded\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_ds  = NERDataset(train_df, tokenizer, MAX_LEN)\n",
    "weights   = np.where(train_df[\"__has_minority\"].values, MINORITY_BOOST, 1.0).astype(\"float32\")\n",
    "sampler   = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "collate   = Collator(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False,\n",
    "    collate_fn=collate, num_workers=2, pin_memory=torch.cuda.is_available(), persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ячейка 4. Модель (XLM-R → Linear → CRF) =================================\n",
    "class TransformerCRF(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, bias_scale: float=1.0):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.emissions = nn.Linear(hidden, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        self.bias_scale = bias_scale\n",
    "    def forward(self, input_ids, attention_mask, labels=None, feat_bias=None):\n",
    "        h = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.emissions(h)\n",
    "        if feat_bias is not None:\n",
    "            logits = logits + self.bias_scale * feat_bias.to(logits.dtype)\n",
    "        logits = logits[:,1:-1,:]                       # вырезаем <s> </s>\n",
    "        mask_seq = attention_mask[:,1:-1].bool()\n",
    "        if labels is not None:\n",
    "            gold = labels[:,1:-1]\n",
    "            tag = gold.clone(); tag[gold==-100] = 0\n",
    "            mask_gold = (gold != -100)\n",
    "            nll = -self.crf(logits, tag.long(), mask=mask_gold, reduction='mean')\n",
    "            return nll\n",
    "        else:\n",
    "            return self.crf.decode(logits, mask=mask_seq)\n",
    "\n",
    "model = TransformerCRF(MODEL_NAME, num_labels=len(LABELS), bias_scale=BIAS_SCALE).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01, fused=torch.cuda.is_available())\n",
    "steps_per_epoch = max(1, math.ceil(len(train_ds)/BATCH_SIZE))\n",
    "total_steps = EPOCHS * steps_per_epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, int(WARMUP_RATIO*total_steps), total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ячейка 5. Обучение (лог/прогресс) + отчёт по train ======================\n",
    "@torch.no_grad()\n",
    "def eval_on_train(model, loader_eval):\n",
    "    model.eval(); y_true, y_pred = [], []\n",
    "    pbar = tqdm(loader_eval, desc=\"eval[train]\", leave=False)\n",
    "    for batch in pbar:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        paths = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], feat_bias=batch[\"feat_bias\"])\n",
    "        gold  = batch[\"labels\"].cpu().tolist()\n",
    "        for i, path in enumerate(paths):\n",
    "            inner = gold[i][1:-1]\n",
    "            first_mask = [x != -100 for x in inner]\n",
    "            true_seq = [ID2LABEL[x] for x,m in zip(inner, first_mask) if m]\n",
    "            pred_seq = [ID2LABEL[p] for p,m in zip(path,  first_mask) if m]\n",
    "            y_true.append(true_seq); y_pred.append(pred_seq)\n",
    "    rep = classification_report(y_true, y_pred, digits=3, zero_division=0)\n",
    "    macro = f1_score(y_true, y_pred)  # macro-F1 без 'O'\n",
    "    return macro, rep\n",
    "\n",
    "best_f1 = -1.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); total_loss = 0.0; t0 = time.time(); tokens_seen = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"train[{epoch}/{EPOCHS}]\", leave=True)\n",
    "    for step, batch in enumerate(pbar, start=1):\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            loss = model(**batch)\n",
    "        scaler.scale(loss).backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        tokens_seen += int(batch['attention_mask'][:,1:-1].sum().item())\n",
    "        tokps = tokens_seen / max(1e-6, time.time()-t0)\n",
    "        pbar.set_postfix({\"loss\": f\"{total_loss/step:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\", \"tok/s\": f\"{tokps:.0f}\"})\n",
    "    # отчёт по train (без отдельного валида)\n",
    "    eval_loader = DataLoader(train_ds, batch_size=max(32,BATCH_SIZE), shuffle=False, collate_fn=collate)\n",
    "    f1m, rep = eval_on_train(model, eval_loader)\n",
    "    print(rep)\n",
    "    if f1m > best_f1:\n",
    "        best_f1 = f1m\n",
    "        torch.save(model.state_dict(), os.path.join(OUT_DIR, \"model.pt\"))\n",
    "        tokenizer.save_pretrained(os.path.join(OUT_DIR, \"tokenizer\"))\n",
    "        with open(os.path.join(OUT_DIR, \"labels.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            for l in LABELS: f.write(l+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf16f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(SUB_IN, sep=';', dtype=str, keep_default_na=False)\n",
    "assert \"sample\" in sub_df.columns, \"В файле сабмишна должна быть колонка 'sample'\"\n",
    "\n",
    "# подгружаем лучшие веса (если не загружены)\n",
    "best_path = os.path.join(OUT_DIR, \"model.pt\")\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "texts = sub_df[\"sample\"].astype(str).tolist()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_texts(texts: List[str]) -> List[str]:\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    encs, wids, toks_info = [], [], []\n",
    "    for tx in texts:\n",
    "        toks = ws_tokens_with_offsets(tx)\n",
    "        words = [t for t,_,_ in toks]\n",
    "        enc = tokenizer(words, is_split_into_words=True, truncation=True, max_length=MAX_LEN)\n",
    "        encs.append(enc); wids.append(enc.word_ids()); toks_info.append(toks)\n",
    "\n",
    "    BS = max(32, BATCH_SIZE)\n",
    "    for i in tqdm(range(0, len(encs), BS), desc=\"infer\", leave=True):\n",
    "        part  = encs[i:i+BS]\n",
    "        wpart = wids[i:i+BS]\n",
    "        tpart = toks_info[i:i+BS]\n",
    "\n",
    "        padded = tokenizer.pad(part, padding=True, return_tensors=\"pt\")\n",
    "        inp = {k: v.to(device) for k,v in padded.items()}\n",
    "\n",
    "        # мягкие подсказки (нуджи) только на первые сабтокены слов\n",
    "        feat_bias = torch.zeros((inp[\"input_ids\"].size(0), inp[\"input_ids\"].size(1), len(LABELS)),\n",
    "                                dtype=torch.float32, device=device)\n",
    "        for bi, wi in enumerate(wpart):\n",
    "            words = [t for t,_,_ in tpart[bi]]\n",
    "            feats = token_feature_bias(words)\n",
    "            prev = None\n",
    "            for pos, wid in enumerate(wi):\n",
    "                if wid is None:\n",
    "                    continue\n",
    "                if wid != prev:\n",
    "                    vec = [0.0]*len(LABELS)\n",
    "                    for k,v in feats[wid].items():\n",
    "                        if k in LABEL2ID: vec[LABEL2ID[k]] += v\n",
    "                    feat_bias[bi, pos, :] = torch.tensor(vec, device=device)\n",
    "                prev = wid\n",
    "\n",
    "        paths = model(input_ids=inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], feat_bias=feat_bias)\n",
    "\n",
    "        # в word-метки и строку с кортежами (start, end, 'TAG') по словам\n",
    "        for k, path in enumerate(paths):\n",
    "            wi   = wpart[k]\n",
    "            toks = tpart[k]\n",
    "            first = []\n",
    "            prev = None\n",
    "            for wid in wi[1:-1]:\n",
    "                if wid is None:\n",
    "                    first.append(False)\n",
    "                elif wid != prev:\n",
    "                    first.append(True)\n",
    "                else:\n",
    "                    first.append(False)\n",
    "                prev = wid\n",
    "\n",
    "            word_labels = [ID2LABEL[p] for p,m in zip(path, first) if m]\n",
    "            # выравниваем длину под число слов\n",
    "            if len(word_labels) < len(toks):\n",
    "                word_labels += [\"O\"] * (len(toks) - len(word_labels))\n",
    "            elif len(word_labels) > len(toks):\n",
    "                word_labels = word_labels[:len(toks)]\n",
    "\n",
    "            spans_str = \"[\" + \", \".join(f\"({s}, {e}, '{lab}')\" for (_,s,e), lab in zip(toks, word_labels)) + \"]\"\n",
    "            preds.append(spans_str)\n",
    "\n",
    "    return preds\n",
    "\n",
    "pred_ann = infer_texts(texts)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": np.arange(1, len(sub_df) + 1),\n",
    "    \"search_query\": sub_df[\"sample\"].astype(str).values,\n",
    "    \"annotation\": pred_ann\n",
    "})\n",
    "submission.to_csv(SUB_OUT, sep=';', index=False)\n",
    "print(submission.head(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
