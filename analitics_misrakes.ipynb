{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# ФАЙЛ ДЛЯ ПОИСКА ОШИБОК ПРЕДСКАЗАНИЙ (Colab)\n",
    "# ===========================\n",
    "# Установка зависимостей (без шума)\n",
    "!pip -q install transformers==4.44.2 tokenizers==0.19.1 pytorch-crf==0.7.2 seqeval==1.2.2 pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3da42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, ast, json, random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Подключаем Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# КОНФИГ\n",
    "# ===========================\n",
    "MODEL_DIR = \"/content/drive/MyDrive/var_model_3\"   # содержит: model.pt, labels.txt, tokenizer/\n",
    "TRAIN_CSV = \"/content/drive/MyDrive/train.csv\"     # содержит: sample;annotation\n",
    "OUT_DIR   = \"/content/drive/MyDrive/var_model_3_analysis\"\n",
    "ERRORS_CSV = os.path.join(OUT_DIR, \"errors.csv\")\n",
    "\n",
    "MAX_LEN    = 96        # длина последовательности (сабтокены)\n",
    "BIAS_SCALE = 0.5       # сила мягких подсказок\n",
    "SEED       = 42\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7476aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Утилиты: токены по словам, мягкие подсказки, парсер annotation\n",
    "# ===========================\n",
    "WS_RE = re.compile(r\"\\S+\")\n",
    "NUM_RE = re.compile(r\"^\\d+[\\d,.]*$\")\n",
    "ASCII_RE = re.compile(r\"^[A-Za-z]+$\")\n",
    "UNITS = {\"л\",\"литр\",\"литра\",\"литров\",\"мл\",\"гр\",\"г\",\"кг\",\"шт\",\"килограммов\", \"грамм\", \"граммов\", \"миллилитров\"}\n",
    "PCT_WORDS = {\"%\",\"процент\",\"проц\", \"процентов\"}\n",
    "ENTITY_TAGS = {\"BRAND\",\"TYPE\",\"VOLUME\",\"PERCENT\"}\n",
    "\n",
    "def ws_tokens_with_offsets(text: str) -> List[Tuple[str,int,int]]:\n",
    "    return [(m.group(0), m.start(), m.end()) for m in WS_RE.finditer(text or \"\")]\n",
    "\n",
    "def token_feature_bias(words: List[str]) -> List[Dict[str,float]]:\n",
    "    feats = []\n",
    "    for i,w in enumerate(words):\n",
    "        wl = w.lower()\n",
    "        f: Dict[str,float] = {}\n",
    "        is_num   = bool(NUM_RE.match(wl))\n",
    "        is_ascii = bool(ASCII_RE.match(w))\n",
    "        prev = words[i-1].lower() if i-1>=0 else \"\"\n",
    "        nxt  = words[i+1].lower() if i+1<len(words) else \"\"\n",
    "        # BRAND: латиница (не юнит)\n",
    "        if is_ascii and wl not in UNITS:\n",
    "            f[\"B-BRAND\"] = f.get(\"B-BRAND\", 0.0) + 0.25\n",
    "        # PERCENT: число рядом со словом/знаком процента\n",
    "        if is_num and (nxt in PCT_WORDS or prev in PCT_WORDS or \"%\" in nxt or \"%\" in prev):\n",
    "            f[\"B-PERCENT\"] = f.get(\"B-PERCENT\", 0.0) + 0.35\n",
    "            f[\"I-PERCENT\"] = f.get(\"I-PERCENT\", 0.0) + 0.15\n",
    "        # VOLUME: число рядом с юнитом\n",
    "        if is_num and (nxt in UNITS):\n",
    "            f[\"B-VOLUME\"] = f.get(\"B-VOLUME\", 0.0) + 0.35\n",
    "        if wl in UNITS and bool(NUM_RE.match(prev)):\n",
    "            f[\"I-VOLUME\"] = f.get(\"I-VOLUME\", 0.0) + 0.20\n",
    "        feats.append(f)\n",
    "    return feats\n",
    "\n",
    "def safe_parse_annotation(s: str) -> List[Tuple[int,int,str]]:\n",
    "    if s is None: return []\n",
    "    txt = str(s).strip()\n",
    "    if txt == \"\" or txt.lower() in {\"nan\",\"none\",\"null\"}: return []\n",
    "    out: List[Tuple[int,int,str]] = []\n",
    "    def _push(a,b,t):\n",
    "        try:\n",
    "            a,b = int(a), int(b)\n",
    "            if b > a:\n",
    "                if t == \"O\": out.append((a,b,\"O\")); return\n",
    "                tag = str(t).split(\"-\")[-1].upper()\n",
    "                pref = str(t).split(\"-\")[0] if \"-\" in str(t) else \"B\"\n",
    "                pref = \"I\" if pref.upper().startswith(\"I\") else \"B\"\n",
    "                if tag in ENTITY_TAGS: out.append((a,b,f\"{pref}-{tag}\"))\n",
    "        except: pass\n",
    "    # literal_eval\n",
    "    try:\n",
    "        v = ast.literal_eval(txt)\n",
    "        if isinstance(v, list):\n",
    "            for it in v:\n",
    "                if isinstance(it, (list,tuple)) and len(it)>=3:\n",
    "                    _push(it[0], it[1], it[2])\n",
    "                elif isinstance(it, dict):\n",
    "                    _push(it.get(\"start\",0), it.get(\"end\",0), it.get(\"label\", it.get(\"tag\",\"O\")))\n",
    "            return sorted(out, key=lambda z:(z[0],z[1]))\n",
    "    except: pass\n",
    "    # json\n",
    "    try:\n",
    "        v = json.loads(txt)\n",
    "        if isinstance(v, list):\n",
    "            for it in v:\n",
    "                if isinstance(it, dict):\n",
    "                    _push(it.get(\"start\",0), it.get(\"end\",0), it.get(\"label\", it.get(\"tag\",\"O\")))\n",
    "        return sorted(out, key=lambda z:(z[0],z[1]))\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def spans_to_bio_on_words(text: str, spans: List[Tuple[int,int,str]]) -> List[str]:\n",
    "    \"\"\"Проецируем символьные спаны на слова через IoU и строим BIO на уровне слов.\"\"\"\n",
    "    toks = ws_tokens_with_offsets(text)\n",
    "    labels = [\"O\"]*len(toks)\n",
    "    for i, (_, ts, te) in enumerate(toks):\n",
    "        best_iou, best_tag = 0.0, \"O\"\n",
    "        for (a,b,t) in spans:\n",
    "            inter = max(0, min(te,b)-max(ts,a))\n",
    "            union = max(te,b)-min(ts,a)\n",
    "            iou = inter/union if union>0 else 0.0\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_tag = t.split(\"-\")[-1] if t!=\"O\" else \"O\"\n",
    "        if best_iou > 0 and best_tag in ENTITY_TAGS:\n",
    "            labels[i] = (\"I-\" if i>0 and labels[i-1].endswith(best_tag) else \"B-\") + best_tag\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Архитектура + загрузка модели/токенайзера\n",
    "# ===========================\n",
    "# labels\n",
    "labels_path = os.path.join(MODEL_DIR, \"labels.txt\")\n",
    "assert os.path.exists(labels_path), f\"labels.txt not found in {MODEL_DIR}\"\n",
    "with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    LABELS = [ln.strip() for ln in f if ln.strip()]\n",
    "LABEL2ID = {l:i for i,l in enumerate(LABELS)}\n",
    "ID2LABEL = {i:l for l,i in LABEL2ID.items()}\n",
    "\n",
    "# tokenizer (из локальной папки)\n",
    "tok_dir = os.path.join(MODEL_DIR, \"tokenizer\")\n",
    "assert os.path.isdir(tok_dir), f\"tokenizer dir not found: {tok_dir}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tok_dir)\n",
    "\n",
    "# backbone по имени модели, а не из папки токенайзера\n",
    "BASE_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "class TransformerCRF(nn.Module):\n",
    "    def __init__(self, model_name: str, num_labels: int, bias_scale: float=0.5):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.emissions = nn.Linear(hidden, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        self.bias_scale = float(bias_scale)\n",
    "    def forward(self, input_ids, attention_mask, labels=None, feat_bias=None):\n",
    "        h = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        logits = self.emissions(h)\n",
    "        if feat_bias is not None:\n",
    "            logits = logits + self.bias_scale * feat_bias.to(logits.dtype)\n",
    "        logits = logits[:,1:-1,:]\n",
    "        mask_seq = attention_mask[:,1:-1].bool()\n",
    "        if labels is not None:\n",
    "            gold = labels[:,1:-1]\n",
    "            tags = gold.clone(); tags[gold == -100] = 0\n",
    "            mask_gold = (gold != -100)\n",
    "            return -self.crf(logits, tags.long(), mask=mask_gold, reduction=\"mean\")\n",
    "        else:\n",
    "            return self.crf.decode(logits, mask=mask_seq)\n",
    "\n",
    "model = TransformerCRF(BASE_NAME, num_labels=len(LABELS), bias_scale=BIAS_SCALE).to(DEVICE)\n",
    "state = torch.load(os.path.join(MODEL_DIR, \"model.pt\"), map_location=DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"Model loaded:\", BASE_NAME, \"| labels:\", len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a02ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Загрузка train и подготовка \"истины\"\n",
    "# ===========================\n",
    "df = pd.read_csv(TRAIN_CSV, sep=';', dtype=str, keep_default_na=False)\n",
    "assert {\"sample\",\"annotation\"}.issubset(df.columns), \"Нужны колонки sample;annotation\"\n",
    "\n",
    "df[\"__spans\"]  = df[\"annotation\"].map(safe_parse_annotation)\n",
    "df[\"__tokens\"] = df[\"sample\"].astype(str).map(lambda t: [w for w,_,_ in ws_tokens_with_offsets(t)])\n",
    "df[\"__ws_bio\"] = [spans_to_bio_on_words(t, s) for t,s in zip(df[\"sample\"], df[\"__spans\"])]\n",
    "\n",
    "ok = all(len(a)==len(b) for a,b in zip(df[\"__tokens\"], df[\"__ws_bio\"]))\n",
    "print(\"Word/BIO lengths match:\", ok, \"| rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Инференс на всём train (BIO на уровне слов)\n",
    "# ===========================\n",
    "@torch.no_grad()\n",
    "def predict_words(words_list: List[List[str]]) -> List[List[str]]:\n",
    "    preds_all: List[List[str]] = []\n",
    "    BS = 64\n",
    "    for i in tqdm(range(0, len(words_list), BS), desc=\"infer(train)\"):\n",
    "        batch_words = words_list[i:i+BS]\n",
    "        encs, wids, feats_list = [], [], []\n",
    "        for words in batch_words:\n",
    "            enc = tokenizer(words, is_split_into_words=True, truncation=True, max_length=MAX_LEN)\n",
    "            encs.append(enc); wids.append(enc.word_ids()); feats_list.append(token_feature_bias(words))\n",
    "        padded = tokenizer.pad(encs, padding=True, return_tensors=\"pt\")\n",
    "        inp = {k:v.to(DEVICE) for k,v in padded.items()}\n",
    "        feat_bias = torch.zeros((inp[\"input_ids\"].size(0), inp[\"input_ids\"].size(1), len(LABELS)), dtype=torch.float32, device=DEVICE)\n",
    "        # нуджи на первые сабтокены слов\n",
    "        for bi, wi in enumerate(wids):\n",
    "            feats = feats_list[bi]\n",
    "            prev = None\n",
    "            for pos, wid in enumerate(wi):\n",
    "                if wid is None: continue\n",
    "                if wid != prev:\n",
    "                    vec = [0.0]*len(LABELS)\n",
    "                    for k,v in feats[wid].items():\n",
    "                        if k in LABEL2ID: vec[LABEL2ID[k]] += v\n",
    "                    feat_bias[bi, pos, :] = torch.tensor(vec, dtype=torch.float32, device=DEVICE)\n",
    "                prev = wid\n",
    "        paths = model(input_ids=inp[\"input_ids\"], attention_mask=inp[\"attention_mask\"], feat_bias=feat_bias)\n",
    "        # только первые сабтокены → метки на слова\n",
    "        for bi, path in enumerate(paths):\n",
    "            wi = wids[bi]\n",
    "            first = []; prev = None\n",
    "            for wid in wi[1:-1]:\n",
    "                if wid is None: first.append(False)\n",
    "                elif wid != prev: first.append(True)\n",
    "                else: first.append(False)\n",
    "                prev = wid\n",
    "            word_labels = [ID2LABEL[p] for p, keep in zip(path, first) if keep]\n",
    "            words = words_list[i+bi]\n",
    "            if len(word_labels) < len(words):\n",
    "                word_labels += [\"O\"]*(len(words)-len(word_labels))\n",
    "            elif len(word_labels) > len(words):\n",
    "                word_labels = word_labels[:len(words)]\n",
    "            preds_all.append(word_labels)\n",
    "    return preds_all\n",
    "\n",
    "pred_bio = predict_words(df[\"__tokens\"].tolist())\n",
    "df[\"__pred_bio\"] = pred_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Метрики (seqeval) и отчёт\n",
    "# ===========================\n",
    "true_seqs = df[\"__ws_bio\"].tolist()\n",
    "pred_seqs = df[\"__pred_bio\"].tolist()\n",
    "\n",
    "print(\"\\n=== Classification report (seqeval, без 'O') ===\")\n",
    "print(classification_report(true_seqs, pred_seqs, digits=3, zero_division=0))\n",
    "print(\"macro-F1:\", f1_score(true_seqs, pred_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dcced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Сбор ошибок: где расходятся BIO-метки\n",
    "# ===========================\n",
    "def diff_indices(y_true: List[str], y_pred: List[str]) -> List[int]:\n",
    "    return [i for i,(a,b) in enumerate(zip(y_true, y_pred)) if a != b]\n",
    "\n",
    "rows = []\n",
    "for idx, (text, toks, y_t, y_p) in enumerate(zip(df[\"sample\"], df[\"__tokens\"], df[\"__ws_bio\"], df[\"__pred_bio\"])):\n",
    "    diffs = diff_indices(y_t, y_p)\n",
    "    if diffs:\n",
    "        # компактное превью: позиция:токен(истина→пред)\n",
    "        preview = \", \".join(f\"{i}:{toks[i]}({y_t[i]}→{y_p[i]})\" for i in diffs[:25])\n",
    "        rows.append({\n",
    "            \"row_id\": idx,\n",
    "            \"sample\": text,\n",
    "            \"tokens\": \" \".join(toks),\n",
    "            \"true_bio\": \" \".join(y_t),\n",
    "            \"pred_bio\": \" \".join(y_p),\n",
    "            \"diff_cnt\": len(diffs),\n",
    "            \"diff_preview\": preview,\n",
    "            \"has_VP_true\": any(x.endswith((\"VOLUME\",\"PERCENT\")) for x in y_t if x!=\"O\"),\n",
    "            \"has_VP_pred\": any(x.endswith((\"VOLUME\",\"PERCENT\")) for x in y_p if x!=\"O\"),\n",
    "        })\n",
    "\n",
    "errors_df = pd.DataFrame(rows).sort_values(\"diff_cnt\", ascending=False).reset_index(drop=True)\n",
    "errors_df.to_csv(ERRORS_CSV, index=False)\n",
    "print(f\"\\nSaved errors → {ERRORS_CSV}\")\n",
    "print(f\"Ошибочных строк: {len(errors_df)} из {len(df)}\")\n",
    "display(errors_df.head(15))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
