{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 0) Загрузка ==========\n",
    "def load_df(path: str = \".data/train.csv\") -> pd.DataFrame:\n",
    "    \"\"\"Ожидает CSV с колонками: sample;annotation\"\"\"\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    # приводим к строкам\n",
    "    df[\"sample\"] = df[\"sample\"].astype(str)\n",
    "    df[\"annotation\"] = df[\"annotation\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========== 1) Парсер annotation ==========\n",
    "def parse_annotation(cell: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"'[(0, 7, \\\"B-TYPE\\\"), ...]' -> [(0,7,'B-TYPE'), ...]; ничего не нормализуем.\"\"\"\n",
    "    if not isinstance(cell, str) or not cell.strip():\n",
    "        return []\n",
    "    try:\n",
    "        items = ast.literal_eval(cell)\n",
    "        return [(int(s), int(e), str(t)) for s, e, t in items]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ========== 2) Токенизация с индексами ==========\n",
    "@dataclass\n",
    "class Token:\n",
    "    text: str\n",
    "    start: int  # включительно\n",
    "    end: int    # исключительно\n",
    "\n",
    "_TOKEN_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def tokenize_with_offsets(text: str) -> List[Token]:\n",
    "    \"\"\"'абрикосы 500 г' -> [Token('абрикосы',0,8), Token('500',9,12), Token('г',13,14)]\"\"\"\n",
    "    return [Token(m.group(0), m.start(), m.end()) for m in _TOKEN_RE.finditer(text or \"\")]\n",
    "\n",
    "\n",
    "# ========== 3) Спаны -> BIO (по токенам) ==========\n",
    "def spans_to_token_bio(tokens: List[Token],\n",
    "                       spans: List[Tuple[int, int, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Раздаём каждой позиции-токену BIO-метку из символьных спанов.\n",
    "    Правило пересечения: >= 1 символа.\n",
    "    \"\"\"\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for s, e, tag in spans:\n",
    "        ent_type = tag.split(\"-\", 1)[-1] if \"-\" in tag else tag\n",
    "        first = True\n",
    "        for i, t in enumerate(tokens):\n",
    "            if not (t.end <= s or e <= t.start):\n",
    "                labels[i] = f\"{'B' if first else 'I'}-{ent_type}\"\n",
    "                first = False\n",
    "    return labels\n",
    "\n",
    "\n",
    "# ========== 4) Признаки токена (окно ±2) ==========\n",
    "def _shape(w: str) -> str:\n",
    "    return \"\".join(\"X\" if c.isalpha() else \"d\" if c.isdigit() else \"_\" for c in w)\n",
    "\n",
    "def _char_ngrams(w: str, n: int = 3, cap: int = 5) -> List[str]:\n",
    "    \"\"\"Символьные n-граммы (по умолчанию триграммы), не более cap штук на токен.\"\"\"\n",
    "    w2 = f\"^{w}$\"\n",
    "    grams = [w2[i:i+n].lower() for i in range(len(w2) - n + 1)]\n",
    "    return grams[:cap]\n",
    "\n",
    "def token_features(tokens: List[Token], i: int) -> Dict[str, Any]:\n",
    "    \"\"\"Признаки токена с окном контекста ±2 и символьными триграммами.\"\"\"\n",
    "    w = tokens[i].text\n",
    "    wl = w.lower()\n",
    "    feats: Dict[str, Any] = {\n",
    "        \"w.lower\": wl,\n",
    "        \"shape\": _shape(w),\n",
    "        \"is_digit\": w.isdigit(),\n",
    "        \"has_digit\": any(c.isdigit() for c in w),\n",
    "        \"has_pct\": (\"%\" in w) or (\"процент\" in wl) or (\"percent\" in wl) or (\"pct\" in wl),\n",
    "        \"has_hyphen\": \"-\" in w,\n",
    "        \"has_dot\": \".\" in w,\n",
    "        \"has_comma\": \",\" in w,\n",
    "        \"is_latin\": bool(re.search(r\"[A-Za-z]\", w)),\n",
    "        \"is_cyrillic\": bool(re.search(r\"[А-Яа-яЁё]\", w)),\n",
    "        \"is_upper\": w.isupper(),\n",
    "        \"is_title\": w.istitle(),\n",
    "        \"len\": len(w),\n",
    "        \"len_bin\": 0 if len(w) <= 2 else 1 if len(w) <= 5 else 2 if len(w) <= 10 else 3,\n",
    "        \"BOS\": i == 0,\n",
    "        \"EOS\": i == len(tokens) - 1,\n",
    "    }\n",
    "    # символьные триграммы (до 5 штук)\n",
    "    for j, g in enumerate(_char_ngrams(w, 3, cap=5)):\n",
    "        feats[f\"tri[{j}]\"] = g\n",
    "\n",
    "    # контекст ±1 и ±2\n",
    "    def add_ctx(j: int, p: str):\n",
    "        wj = tokens[j].text\n",
    "        feats.update({\n",
    "            f\"{p}:w.lower\": wj.lower(),\n",
    "            f\"{p}:shape\": _shape(wj),\n",
    "            f\"{p}:is_upper\": wj.isupper(),\n",
    "            f\"{p}:has_digit\": any(c.isdigit() for c in wj),\n",
    "            f\"{p}:is_latin\": bool(re.search(r\"[A-Za-z]\", wj)),\n",
    "            f\"{p}:is_cyr\": bool(re.search(r\"[А-Яа-яЁё]\", wj)),\n",
    "        })\n",
    "        # триграммы у соседей — по 2 шт, чтобы не раздувать пространство\n",
    "        for k, g in enumerate(_char_ngrams(wj, 3, cap=2)):\n",
    "            feats[f\"{p}:tri[{k}]\"] = g\n",
    "\n",
    "    if i - 1 >= 0: add_ctx(i - 1, \"-1\")\n",
    "    if i - 2 >= 0: add_ctx(i - 2, \"-2\")\n",
    "    if i + 1 < len(tokens): add_ctx(i + 1, \"+1\")\n",
    "    if i + 2 < len(tokens): add_ctx(i + 2, \"+2\")\n",
    "    return feats\n",
    "\n",
    "def sent2features(tokens: List[Token]) -> List[Dict[str, Any]]:\n",
    "    return [token_features(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "# ====== СИЛЬНЫЕ РЕГЕКСЫ ДЛЯ ЧИСЕЛ: VOLUME / PERCENT ======\n",
    "# Число: 12, 12.5, 12,5\n",
    "_NUM = r\"\\d+(?:[.,]\\d+)?\"\n",
    "\n",
    "# Единицы объёма/веса/штук с падежами и частыми опечатками:\n",
    "# - миллилитр: \"мл\", \"миллилитр/миллилитров/миллилитра\", опечатки \"милилитр\", \"милллитр\", \"миллил\"\n",
    "# - литр: \"л\", \"литр/литров/литра\"\n",
    "# - грамм: \"г\", \"гр\", \"грамм/граммов/грамма\", \"грам\", \"грамов\" (опечатка)\n",
    "# - килограмм: \"кг\"\n",
    "# - штука: \"шт\", \"штук/штуки/штука\", опечатки вида \"штцк\", \"штк\"\n",
    "_UNIT_VOL = r\"\"\"\n",
    "(?ix)\n",
    "(\n",
    "    # миллилитры\n",
    "    мл\n",
    "    | ми?л+и?л+итр\\w*          # миллилитр*, милилитр*, \"милллитр*\"\n",
    "    | миллил\\w*                # укороченные/с ошибками \"миллил...\"\n",
    "    # литры\n",
    "    | л\\b\n",
    "    | литр\\w*\n",
    "    # граммы\n",
    "    | г\\b\n",
    "    | гр\\b\n",
    "    | грамм\\w*\n",
    "    | грам\\w*                  # грам/грамов\n",
    "    # килограммы\n",
    "    | кг\\b\n",
    "    # штуки (в т.ч. частые опечатки)\n",
    "    | шт\\w*                    # шт, шт., штцк, штк...\n",
    "    | штук\\w*\n",
    "    | штук\\w*\n",
    "    | штук[аио]\\w*\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Проценты: знак % или слово \"процент\" в падежах + англ. percent/pct, лёгкие опечатки \"процен\" / \"проценты\"\n",
    "_WORD_PCT = r\"(?:(?:процен[тт]\\w*)|percent(?:age)?|pct)\"\n",
    "\n",
    "VOLUME_RE = re.compile(\n",
    "    rf\"\"\"(?ix)\n",
    "    (?<!\\w)                                # не буква/цифра слева\n",
    "    ({_NUM})                               # число\n",
    "    \\s*[-]?\\s*                             # пробел/дефис между числом и юнитом\n",
    "    {_UNIT_VOL}                            # единица измерения\n",
    "    \\b\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "PERCENT_RE = re.compile(\n",
    "    rf\"\"\"(?ix)\n",
    "    (?:\n",
    "        (?<!\\w)({_NUM})\\s*%                # 2.5%\n",
    "        |\n",
    "        (?<!\\w)({_NUM})\\s*{_WORD_PCT}\\b    # 2,5 процента/процентов/percent/pct\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def find_rule_spans(text: str) -> List[Tuple[int, int, str]]:\n",
    "    spans = []\n",
    "    for m in VOLUME_RE.finditer(text):\n",
    "        spans.append((m.start(), m.end(), \"B-VOLUME\"))\n",
    "    for m in PERCENT_RE.finditer(text):\n",
    "        spans.append((m.start(), m.end(), \"B-PERCENT\"))\n",
    "    return spans\n",
    "\n",
    "# Валидация найденных спанов: выкидываем явные фальши\n",
    "_VOL_TXT = re.compile(_UNIT_VOL, re.I | re.X)\n",
    "_PCT_TXT = re.compile(_WORD_PCT + r\"|%\", re.I)\n",
    "\n",
    "def post_validate_spans(text: str, spans: List[Tuple[int, int, str]]) -> List[Tuple[int, int, str]]:\n",
    "    ok = []\n",
    "    for s, e, t in spans:\n",
    "        frag = text[s:e]\n",
    "        ent = t.split(\"-\", 1)[-1]\n",
    "        if ent == \"VOLUME\" and not _VOL_TXT.search(frag):\n",
    "            continue\n",
    "        if ent == \"PERCENT\" and not _PCT_TXT.search(frag):\n",
    "            continue\n",
    "        ok.append((s, e, t))\n",
    "    return ok\n",
    "\n",
    "# ========== 5) CRF ==========\n",
    "def init_crf(algorithm=\"lbfgs\",\n",
    "        c1=0.01,\n",
    "        c2=0.05,\n",
    "        max_iterations=150,\n",
    "        all_possible_transitions=True,\n",
    "        all_possible_states=False,\n",
    "        verbose=False,) -> CRF:\n",
    "    \"\"\"CRF для BIO, 300 итераций.\"\"\"\n",
    "    return CRF(\n",
    "        algorithm=algorithm,\n",
    "        c1=c1,\n",
    "        c2=c2,\n",
    "        max_iterations=max_iterations,\n",
    "        all_possible_transitions=all_possible_transitions,\n",
    "        all_possible_states=all_possible_states,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def build_xy(df: pd.DataFrame):\n",
    "    \"\"\"Из df(sample, annotation) делает X (фичи по токенам) и y (BIO по токенам).\"\"\"\n",
    "    X, y = [], []\n",
    "    for row in df.itertuples(index=False):\n",
    "        text = str(row.sample)\n",
    "        spans = parse_annotation(row.annotation)\n",
    "        toks = tokenize_with_offsets(text)\n",
    "        X.append(sent2features(toks))\n",
    "        y.append(spans_to_token_bio(toks, spans))\n",
    "    return X, y\n",
    "\n",
    "def fit_crf(crf: CRF, X, y) -> CRF:\n",
    "    crf.fit(X, y)\n",
    "    return crf\n",
    "\n",
    "\n",
    "# ========== 6) BIO -> символьные спаны ==========\n",
    "def bio_to_spans(tokens: List[Token], labels: List[str]) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"Склеиваем предсказанные BIO в интервалы (start,end,'B-ENTITY') в координатах исходного текста.\"\"\"\n",
    "    spans = []\n",
    "    cur_ent, cur_s, cur_e = None, None, None\n",
    "\n",
    "    def push():\n",
    "        if cur_ent is not None and cur_s is not None and cur_e is not None:\n",
    "            spans.append((cur_s, cur_e, f\"B-{cur_ent}\"))\n",
    "\n",
    "    for tok, lab in zip(tokens, labels):\n",
    "        if not lab or lab == \"O\" or \"-\" not in lab:\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "                cur_ent, cur_s, cur_e = None, None, None\n",
    "            continue\n",
    "        pref, ent = lab.split(\"-\", 1)\n",
    "        if pref == \"B\":\n",
    "            if cur_ent is not None:\n",
    "                push()\n",
    "            cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "        elif pref == \"I\":\n",
    "            if cur_ent == ent:\n",
    "                cur_e = tok.end\n",
    "            else:\n",
    "                if cur_ent is not None:\n",
    "                    push()\n",
    "                cur_ent, cur_s, cur_e = ent, tok.start, tok.end\n",
    "    if cur_ent is not None:\n",
    "        push()\n",
    "    return spans\n",
    "\n",
    "\n",
    "# ========== 7) Инференс одной строки ==========\n",
    "def predict_one(crf: CRF, text: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    1) Правила вытаскивают VOLUME/PERCENT.\n",
    "    2) CRF даёт BIO по всем токенам -> склеиваем в спаны.\n",
    "    3) Для VOLUME/PERCENT приоритет за правилами (модельные пересекающиеся — выкидываем).\n",
    "    4) Итог валидируем пост-фильтром (выкидываем явные фальши).\n",
    "    \"\"\"\n",
    "    toks = tokenize_with_offsets(text)\n",
    "    if not toks:\n",
    "        return []\n",
    "\n",
    "    # 1) Правила\n",
    "    rule_spans = find_rule_spans(text)\n",
    "\n",
    "    # 2) Модель\n",
    "    y_hat = crf.predict([sent2features(toks)])[0]\n",
    "    model_spans = bio_to_spans(toks, y_hat)\n",
    "\n",
    "    # 3) Слияние: числовые сущности → приоритет правил\n",
    "    final = []\n",
    "    for s, e, t in model_spans:\n",
    "        ent = t.split(\"-\", 1)[-1]\n",
    "        if ent in {\"VOLUME\", \"PERCENT\"}:\n",
    "            # если пересекается с правилом того же типа — пропускаем модельный\n",
    "            if any(not (e2 <= s or e <= s2) and t2.endswith(ent) for s2, e2, t2 in rule_spans):\n",
    "                continue\n",
    "        final.append((s, e, f\"B-{ent}\"))\n",
    "    final.extend(rule_spans)\n",
    "\n",
    "    # 4) Сортировка и склейка однотипных пересечений\n",
    "    final.sort(key=lambda z: (z[0], z[1]))\n",
    "    merged = []\n",
    "    for s, e, t in final:\n",
    "        if not merged:\n",
    "            merged.append([s, e, t])\n",
    "        else:\n",
    "            ps, pe, pt = merged[-1]\n",
    "            if pt == t and s <= pe:\n",
    "                merged[-1][1] = max(pe, e)\n",
    "            else:\n",
    "                merged.append([s, e, t])\n",
    "    merged = [(s, e, t) for s, e, t in merged]\n",
    "\n",
    "    # 5) Пост-валидация\n",
    "    merged = post_validate_spans(text, merged)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ========== 8) Метрика strict spans macro-F1 ==========\n",
    "def spans_exact_f1(y_true: List[List[Tuple[int,int,str]]],\n",
    "                   y_pred: List[List[Tuple[int,int,str]]]) -> float:\n",
    "    \"\"\"TP/FP/FN считаем по ПОЛНОМУ совпадению (start,end, type). Усреднение по TYPE/BRAND/VOLUME/PERCENT.\"\"\"\n",
    "    types = [\"TYPE\", \"BRAND\", \"VOLUME\", \"PERCENT\"]\n",
    "    f1s = []\n",
    "    for ent in types:\n",
    "        tp = fp = fn = 0\n",
    "        for t_sp, p_sp in zip(y_true, y_pred):\n",
    "            T = {(s, e) for s, e, tag in t_sp if str(tag).endswith(ent)}\n",
    "            P = {(s, e) for s, e, tag in p_sp if str(tag).endswith(ent)}\n",
    "            I = T & P\n",
    "            tp += len(I); fp += len(P - I); fn += len(T - I)\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "        f1s.append(f1)\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "\n",
    "# ========== 9) Утилита для просмотра ==========\n",
    "def spans_text_view(text: str, spans):\n",
    "    parts = []\n",
    "    for s, e, t in sorted(spans, key=lambda z: (z[0], z[1])):\n",
    "        frag = text[s:e].replace(\"\\n\", \"\\\\n\")\n",
    "        ent = t.split(\"-\", 1)[-1] if \"-\" in t else t\n",
    "        parts.append(f\"{ent}='{frag}'[{s}:{e}]\")\n",
    "    return \" | \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2955f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: .data/submission_pred_crf.csv\n"
     ]
    }
   ],
   "source": [
    "def serialize_spans(spans):\n",
    "    return \"[\" + \", \".join(f\"({s}, {e}, '{t}')\" for s, e, t in spans) + \"]\"\n",
    "\n",
    "def predict_df(crf, df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    assert {\"sample\", \"annotation\"}.issubset(df_in.columns), \"Нужны колонки: sample;annotation\"\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # предсказания\n",
    "    preds = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        text = str(row.sample)\n",
    "        spans = predict_one(crf, text)\n",
    "        preds.append(serialize_spans(spans))\n",
    "\n",
    "    # заменить annotation\n",
    "    df[\"annotation\"] = preds\n",
    "\n",
    "    # переименовать sample -> search_query\n",
    "    df = df.rename(columns={\"sample\": \"search_query\"})\n",
    "\n",
    "    # добавить id с 1\n",
    "    df.insert(0, \"id\", range(1, len(df) + 1))\n",
    "\n",
    "    # порядок колонок\n",
    "    return df[[\"id\", \"search_query\", \"annotation\"]]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SUBM_IN  = \".data/submission.csv\"\n",
    "    SUBM_OUT = \".data/submission_pred_crf.csv\"\n",
    "    MODEL    = \"models/crf_baseline_3.joblib\"\n",
    "\n",
    "    crf = load(MODEL)[\"model\"]\n",
    "\n",
    "    subm = pd.read_csv(SUBM_IN, sep=\";\")\n",
    "    out  = predict_df(crf, subm)\n",
    "    out.to_csv(SUBM_OUT, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {SUBM_OUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
